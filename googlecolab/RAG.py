import os
from huggingface_hub import InferenceClient
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- √âTAPE DE PR√âPARATION (√† faire une seule fois) ---

# 1. Charger notre source de connaissances
with open("projet_secret.txt", "r", encoding="utf-8") as f:
    # On divise le document en "chunks" (ici, par ligne)
    knowledge_chunks = [line.strip() for line in f.readlines() if line.strip()]

# 2. Charger un mod√®le pour cr√©er des "embeddings" (repr√©sentations num√©riques du sens)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# 3. Cr√©er les embeddings pour chaque chunk de notre document
#    C'est notre "base de donn√©es vectorielle" simplifi√©e
knowledge_embeddings = embedding_model.encode(knowledge_chunks)

print("‚úÖ Base de connaissances pr√™te et index√©e.\n")


# --- √âTAPE D'INFERENCE RAG (√† chaque question de l'utilisateur) ---

# La question de l'utilisateur
user_question = "Quel est le budget du projet Albatros et qui le dirige ?"

# 1. RETRIEVAL (R√©cup√©ration)
# On transforme la question en embedding
question_embedding = embedding_model.encode([user_question])

# On calcule la similarit√© entre la question et chaque chunk du document
similarities = cosine_similarity(question_embedding, knowledge_embeddings)

# On trouve le chunk le plus pertinent (celui avec le score le plus haut)
most_relevant_chunk_index = np.argmax(similarities)
context = knowledge_chunks[most_relevant_chunk_index]

# Pour un vrai RAG, on prendrait les 'k' meilleurs chunks
# Ici on prend les 2 meilleurs pour un contexte plus riche
top_2_indices = np.argsort(similarities[0])[-2:]
context = "\n".join([knowledge_chunks[i] for i in top_2_indices])

print(f"üîç Contexte pertinent trouv√© dans les documents :\n---\n{context}\n---\n")

# 2. AUGMENTED GENERATION (G√©n√©ration Augment√©e)
# On cr√©e un prompt qui inclut le contexte et la question
prompt_template = f"""
En te basant UNIQUEMENT sur le contexte suivant, r√©ponds √† la question.

Contexte:
{context}

Question:
{user_question}
"""

print("üí¨ Prompt final envoy√© au LLM :\n---\n" + prompt_template + "\n---\n")

# On initialise le client Hugging Face
client = InferenceClient()

# On envoie le prompt enrichi au LLM
completion = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-V3-0324",
    messages=[
        {"role": "user", "content": prompt_template}
    ],
)

# On affiche la r√©ponse, qui sera maintenant factuellement correcte !
print("ü§ñ R√©ponse du LLM (bas√©e sur le RAG) :\n")
print(completion.choices[0].message.content)
