### Pourquoi le script vous demande une **API‚Äëkey de Weights & Biases (wandb)** ?

`ü§ó‚ÄØTransformers` utilise le **Trainer** qui, d√®s qu‚Äôil d√©tecte la biblioth√®que **wandb** dans votre environnement, active automatiquement le *callback* de suivi d‚Äôexp√©riences :

```python
if is_wandb_available():
    trainer.add_callback(WandbCallback)
```

- **Wandb** est un service en ligne qui vous permet de visualiser‚ÄØ:  
  - pertes et m√©triques pendant l‚Äôentra√Ænement,  
  - courbes d‚Äôapprentissage,  
  - graphiques d‚Äôutilisation du GPU/CPU,  
  - versionnage des mod√®les, etc.  

- Quand le callback est activ√©, le premier appel √† `trainer.train()` lance une **session wandb**.  
  Si vous n‚Äôavez jamais connect√© votre machine √† wandb, le SDK vous demande alors votre **API key** (ou l‚Äôinvite de vous connecter via le navigateur).  

> **En bref** : le code que je vous ai fourni ne n√©cessite pas wandb, mais si le paquet `wandb` est install√©, le Trainer l‚Äôutilise automatiquement, d‚Äôo√π la demande d‚ÄôAPI‚Äëkey.

---

## 1Ô∏è‚É£ D√©sactiver Wandb (solution la plus simple)

### a. Supprimer/ignorer le package wandb
```bash
pip uninstall wandb          # ou simplement ne pas l‚Äôinstaller du tout
```

### b. Ou d√©sactiver le callback depuis le script
Ajoutez **une variable d‚Äôenvironnement** **ou** un argument `report_to` dans `TrainingArguments` :

```python
training_args = TrainingArguments(
    output_dir="./llm-finetuned",
    per_device_train_batch_size=4,
    num_train_epochs=1,
    logging_steps=10,
    save_steps=50,
    save_total_limit=2,
    learning_rate=5e-5,
    fp16=torch.cuda.is_available(),
    report_to=[],                 # <-- NE PAS envoyer de logs √† wandb, tensorboard, etc.
)
```

`report_to` accepte une liste parmi `["tensorboard", "wandb", "mlflow", "comet_ml", "azure_ml", "clearml", "neptune"]`. En mettant `[]` on d√©sactive tout le suivi.

---

## 2Ô∏è‚É£ Garder Wandb mais √©viter l‚Äôinvite d‚ÄôAPI‚Äëkey

Si vous voulez quand m√™me profiter de les visualisations wandb, cr√©ez d‚Äôabord votre compte (gratuit) sur‚ÄØ: <https://wandb.ai/site> puis :

1. **Obtenez votre cl√©** :  
   - Connectez‚Äëvous ‚Üí cliquez sur votre avatar ‚Üí *Settings* ‚Üí *API Keys* ‚Üí *Create new key*.

2. **Enregistrez‚Äëla une fois pour toutes** (cela √©vite l‚Äôinvite √† chaque ex√©cution) :

   ```bash
   wandb login YOUR_API_KEY
   ```

   ou d√©finissez la variable d‚Äôenvironnement :

   ```bash
   export WANDB_API_KEY=YOUR_API_KEY          # Linux/macOS
   set WANDB_API_KEY=YOUR_API_KEY             # Windows CMD
   $env:WANDB_API_KEY="YOUR_API_KEY"          # PowerShell
   ```

3. **(Optionnel) Cr√©ez un projet** pour que les runs y aillent automatiquement :

   ```bash
   export WANDB_PROJECT=llm_finetune_demo   # (ou mettre dans TrainingArguments)
   ```

   Puis, dans le script :

   ```python
   training_args = TrainingArguments(
       ...,
       report_to=["wandb"],   # active explicitement wandb
       run_name="run-1",      # nom du run (facultatif)
   )
   ```

---

## 3Ô∏è‚É£ Exemple complet avec `report_to=[]`

Voici le script pr√©c√©dent l√©g√®rement modifi√© pour **ne jamais appeler wandb**, m√™me s‚Äôil est install√© :

```python
# ---------------------------------------------------------
# train_llm.py ‚Äì fine‚Äëtuning d'un petit LLM sans wandb
# ---------------------------------------------------------

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)

# 1Ô∏è‚É£ Jeu de donn√©es tr√®s petit
train_dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train[:1%]")

# 2Ô∏è‚É£ Tokenizer & mod√®le
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3Ô∏è‚É£ Tokenisation
def tokenize(example):
    tokenized = tokenizer(example["text"], truncation=True, max_length=128)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = train_dataset.map(tokenize, remove_columns=["text"])

# 4Ô∏è‚É£ Arguments d‚Äôentra√Ænement ‚Äì d√©sactivation de wandb
training_args = TrainingArguments(
    output_dir="./llm-finetuned",
    per_device_train_batch_size=4,
    num_train_epochs=1,
    logging_steps=10,
    save_steps=50,
    save_total_limit=2,
    learning_rate=5e-5,
    fp16=torch.cuda.is_available(),
    report_to=[],                     # <‚Äë‚Äë rien n‚Äôest envoy√© √† wandb, tensorboard, ‚Ä¶
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

trainer.train()
trainer.save_model("./llm-finetuned")
tokenizer.save_pretrained("./llm-finetuned")
print("‚úÖ Entra√Ænement termin√©, mod√®le sauvegard√© dans ./llm-finetuned")
```

---

## 4Ô∏è‚É£ R√©capitulatif des solutions

| Situation | Action recommand√©e |
|-----------|--------------------|
| **Vous ne voulez **aucun** suivi** | - Supprimez `wandb` (`pip uninstall wandb`) **ou** ajoutez `report_to=[]` dans `TrainingArguments`. |
| **Vous voulez garder wandb, mais ne plus saisir la cl√©** | - Connectez‚Äëvous une fois avec `wandb login <API_KEY>` ou d√©finissez la variable d‚Äôenvironnement `WANDB_API_KEY`. |
| **Vous voulez garder wandb **et** voir les logs** | - Laissez `report_to=["wandb"]` (ou omettez le param√®tre, c‚Äôest le d√©faut) et assurez‚Äëvous d‚Äôavoir la cl√© configur√©e. |
| **Vous utilisez un serveur CI/CD o√π wandb n‚Äôest pas install√©** | - Aucun probl√®me‚ÄØ: le callback ne s‚Äôactive pas tant que le package n‚Äôest pas pr√©sent. |

---

## 5Ô∏è‚É£ Bonus : Utiliser TensorBoard √† la place (si vous voulez quand m√™me un suivi local)

```python
training_args = TrainingArguments(
    ...,
    report_to=["tensorboard"],      # affichage local uniquement
    logging_dir="./logs",           # dossier o√π seront √©crits les *.log
)
```

Puis lancez :

```bash
tensorboard --logdir ./logs
```

et ouvrez votre navigateur sur `http://localhost:6006`.

---

### üéâ Vous avez maintenant le contr√¥le total :

- **Pas de demande d‚ÄôAPI‚Äëkey** : d√©sactivez wandb ou d√©sinstallez le paquet.  
- **Avec wandb** : connectez‚Äëvous une fois et la cl√© est m√©moris√©e.  

Vous pouvez donc relancer votre script `python train_llm.py` en toute s√©r√©nit√©, sans √™tre interrompu par une fen√™tre d‚Äôauthentification. Bonne exp√©rimentation‚ÄØ! üöÄ
